#  SQL Performance Optimization Project (PostgreSQL)

##  Resumen del Proyecto y Propuesta de Valor

Este proyecto demuestra la experiencia de un Ingeniero de Datos  en la **optimizaci贸n cr铆tica de bases de datos PostgreSQL** bajo condiciones de alto volumen.

**El Problema:** Un reporte de negocios esencial (filtrando 50,000 clientes y agregando 500,000 贸rdenes) estaba sufriendo un cuello de botella. La consulta se basaba en costosos **Sequential Scans** (Exploraciones Secuenciales) y un **Hash Join** ineficiente, resultando en latencias inaceptables.

**La Soluci贸n:** Se identific贸 la falta de indexaci贸n en la clave for谩nea (`ordenes.cliente_id`) y la columna de filtrado (`clientes.region`). Se aplicaron 铆ndices estrat茅gicos y se actualiz贸 el planificador de consultas (`ANALYZE`).

### Beneficios Cuantificables

| M茅trica | ANTES de la Optimizaci贸n | DESPUS de la Optimizaci贸n | MEJORA |
| :--- | :--- | :--- | :--- |
| **Tiempo de Ejecuci贸n** | **759 ms** (0.76 segundos) | **230 ms** (0.23 segundos) | **~70% de Reducci贸n** |
| **Recursos del Servidor** | Reducci贸n del uso de CPU y memoria (menor dependencia del Hash Join). |

---

## 吼 Project Overview and Value Proposition

This project showcases a Senior Data Engineers expertise in **critical PostgreSQL database optimization** under high-volume conditions.

**The Problem:** An essential business report (filtering 50,000 clients and aggregating 500,000 orders) was suffering from a bottleneck. The query relied on costly **Sequential Scans** and an inefficient **Hash Join**, leading to unacceptable latency.

**The Solution:** Identified the lack of indexing on the foreign key (`ordenes.cliente_id`) and the filtering column (`clientes.region`). Applied strategic indexes and updated the query planner (`ANALYZE`).

### Quantifiable Benefits

| Metric | BEFORE Optimization | AFTER Optimization | IMPROVEMENT |
| :--- | :--- | :--- | :--- |
| **Execution Time** | **759 ms** (0.76 seconds) | **230 ms** (0.23 seconds) | **~70% Reduction** |
| **Server Resources** | Reduced CPU and memory usage (less reliance on Hash Join). |

---

## Estructura del Repositorio (Rutas Clave)

## Estructura del Repositorio (Rutas Clave) / Repository Structure

El proyecto est谩 organizado profesionalmente siguiendo el flujo de trabajo de la Ingenier铆a de Datos.

* `performance-optimization-sql/`
    * `.gitignore`
    * `README.md`               <-- Este archivo. / This file.
    * `requirements.txt`        <-- Dependencias de Python. / Python dependencies.
    * `sql/`                    <-- Scripts SQL (La l贸gica de BD)
        * `setup_tablas.sql`    <-- ELT Load: Crea 500k filas y tablas. / Creates 500k rows and tables.
        * `consulta_lenta.sql`  <-- ELT Transform (Before): EXPLAIN ANALYZE lento. / Slow EXPLAIN ANALYZE.
        * `consulta_optimizada.sql` <-- ELT Transform (After): CREATE INDEX y EXPLAIN ANALYZE r谩pido. / CREATE INDEX and fast EXPLAIN ANALYZE.
    * `python/`                 <-- Script de Orquestaci贸n (El Pipeline)
        * `main_pipeline.py`    <-- Ejecuta scripts, mide el rendimiento y genera el gr谩fico final. / Executes scripts, measures performance, and generates the final graph.
    * `assets/`                 <-- Evidencia de la Optimizaci贸n (Capturas/Gr谩ficos)
        * `explain_analyze_antes.png`    <-- Captura del plan de ejecuci贸n lento. / Slow execution plan capture.
        * `explain_analyze_despues.png`  <-- Captura del plan de ejecuci贸n optimizado. / Optimized execution plan capture.
        * `performance_graph.png`        <-- Gr谩fico de barras que demuestra la mejora del ~70%. / Bar chart demonstrating ~70% improvement.
---

## Flujo de Trabajo (Para Pruebas)

Para replicar y validar la optimizaci贸n, se deben ejecutar los scripts en este orden estricto (automatizado por `main_pipeline.py`):

1.  **Reiniciar Tablas:** Ejecutar `sql/setup_tablas.sql` (asegura que no haya 铆ndices).
2.  **Medir ANTES:** Ejecutar `sql/consulta_lenta.sql` para registrar el tiempo base (**759 ms**).
3.  **Aplicar Optimizaci贸n:** Ejecutar la parte `CREATE INDEX` de `sql/consulta_optimizada.sql`.
4.  **Medir DESPUS:** Ejecutar la consulta `EXPLAIN ANALYZE` de `sql/consulta_optimizada.sql` para registrar el tiempo optimizado (**230 ms**).

El script de Python se encarga de estos cuatro pasos autom谩ticamente.